{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline: DWF Optimization with RL & Demand Forecasting\n",
    "This notebook implements a Deep Reinforcement Learning model to dynamically optimize fare incentives using preprocessed and enriched ride-hailing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "        t_i = row['Request to Pickup']\n",
    "        delta = 0.2 * base_fare\n",
    "\n",
    "        # Behavior-based features\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "\n",
    "        cr_input_initial = 1.2 * rank_percentile - 1.3 * RPI\n",
    "        original_CR = 1.0 / (1.0 + np.exp(-cr_input_initial))\n",
    "\n",
    "        # Default: no intervention\n",
    "        fare_adjustment, rider_incentive = 0.0, 0.0\n",
    "\n",
    "        # Only apply RL action if original CR >= 0.5 \n",
    "        if original_CR >= 0.5:\n",
    "            fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "\n",
    "        # DPI based on actual action\n",
    "        action_signal = rider_incentive + abs(fare_adjustment * base_fare)\n",
    "        DPI = np.clip(1.0 - np.exp(-0.6 * action_signal) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # Recalculate CR using full action-aware formula\n",
    "        cr_input = 1.2 * rank_percentile - 1.3 * RPI - 2.0 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # Reward and Cost Logic\n",
    "        base_reward = 1.5 * (1.0 - CR)\n",
    "\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        cost_ratio = cost / (base_fare + 5)\n",
    "        cost_penalty = 0.5 * max(cost_ratio - 0.2, 0.0)\n",
    "        efficiency_bonus = 0.2 if ride_completed and cost_ratio < 0.2 else 0.0\n",
    "\n",
    "        reward = base_reward - cost_penalty + efficiency_bonus\n",
    "        reward = np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"RPI\": RPI,\n",
    "            \"DPI\": DPI,\n",
    "            \"cost\": cost,\n",
    "            \"base_reward\": base_reward,\n",
    "            \"cost_penalty\": cost_penalty,\n",
    "            \"original_CR\": original_CR\n",
    "        }\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure logs directory exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Training env\n",
    "train_env = DummyVecEnv([lambda: RideHailingEnv_DWF(train_df)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_reward=10.0)\n",
    "\n",
    "# Validation env\n",
    "val_env = DummyVecEnv([lambda: RideHailingEnv_DWF(val_df)])\n",
    "val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_tensorboard_logs/DWF_v2/PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 281  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029702832 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.798       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.272        |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.646        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=1042.49 +/- 14.62\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.04e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023334594 |\n",
      "|    clip_fraction        | 6.51e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -1.29        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0923       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.281        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 202  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 30   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031768945 |\n",
      "|    clip_fraction        | 0.00013      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -1.32        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00487      |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.181        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=1077.00 +/- 5.22\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008129883 |\n",
      "|    clip_fraction        | 0.00169     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00344    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0904      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 185   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 198          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033977455 |\n",
      "|    clip_fraction        | 0.00013      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.902       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.000239     |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0609       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074264985 |\n",
      "|    clip_fraction        | 0.00273      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.648       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00787     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0491       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=1104.86 +/- 14.16\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.1e+03      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053206617 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.6         |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00037      |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | -0.0037      |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0493       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 190   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005584511 |\n",
      "|    clip_fraction        | 0.00104     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.436      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0476      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1156.36 +/- 7.72\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.16e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013871161 |\n",
      "|    clip_fraction        | 0.00801     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.384      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0477      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 187   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 108   |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014178989 |\n",
      "|    clip_fraction        | 0.01        |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.357      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0492      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020303577 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.279      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0479      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=1179.08 +/- 5.73\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.18e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012298708 |\n",
      "|    clip_fraction        | 0.00527     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.253      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0475      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 190   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 139   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039851386 |\n",
      "|    clip_fraction        | 0.000618     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.258       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00846     |\n",
      "|    n_updates            | 195          |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0487       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1176.70 +/- 6.70\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.18e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007328211 |\n",
      "|    clip_fraction        | 0.00225     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.191      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0477      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 187   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008518073 |\n",
      "|    clip_fraction        | 0.00195     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 196          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075032287 |\n",
      "|    clip_fraction        | 0.00238      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.154       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.000897     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0478       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=1184.18 +/- 2.45\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.18e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008331429 |\n",
      "|    clip_fraction        | 0.00169     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.182      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 255         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0482      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 191   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 192   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 194        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 199        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00961152 |\n",
      "|    clip_fraction        | 0.00404    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.87      |\n",
      "|    explained_variance   | -0.174     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.00572   |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 0.0487     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1204.59 +/- 1.39\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.2e+03      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125141125 |\n",
      "|    clip_fraction        | 0.00531      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.136       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00695     |\n",
      "|    n_updates            | 285          |\n",
      "|    policy_gradient_loss | -0.00561     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.047        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 188   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 217   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018821523 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.143      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00792    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0483      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=1208.64 +/- 3.02\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.21e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012387974 |\n",
      "|    clip_fraction        | 0.00804     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.123      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00696     |\n",
      "|    n_updates            | 315         |\n",
      "|    policy_gradient_loss | -0.0061     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.047       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 186   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 241   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087084705 |\n",
      "|    clip_fraction        | 0.00228      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | -0.104       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00621     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00445     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.0671       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015709898 |\n",
      "|    clip_fraction        | 0.00977     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 345         |\n",
      "|    policy_gradient_loss | -0.0082     |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.0458      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1208.67 +/- 2.20\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.21e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043349224 |\n",
      "|    clip_fraction        | 0.000456     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | -0.106       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0314       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.0458       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 189   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 270   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 277          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046519395 |\n",
      "|    clip_fraction        | 0.000456     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | -0.0884      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00258     |\n",
      "|    n_updates            | 375          |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.0461       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=1209.95 +/- 5.92\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.21e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 55000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072164065 |\n",
      "|    clip_fraction        | 0.00199      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.0891      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00132      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00498     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.0465       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 188   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 292   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 299          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073440825 |\n",
      "|    clip_fraction        | 0.00228      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | -0.0955      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.0196      |\n",
      "|    n_updates            | 405          |\n",
      "|    policy_gradient_loss | -0.00475     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.0462       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 193        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 306        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01348299 |\n",
      "|    clip_fraction        | 0.00879    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.92      |\n",
      "|    explained_variance   | -0.0835    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000165  |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.00757   |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 0.0453     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1209.17 +/- 1.83\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.21e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036889259 |\n",
      "|    clip_fraction        | 0.000553     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | -0.0785      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0337       |\n",
      "|    n_updates            | 435          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.0458       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 190   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 322   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009570914 |\n",
      "|    clip_fraction        | 0.00247     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0574     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.0454      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=1215.54 +/- 4.80\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.22e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017241323 |\n",
      "|    clip_fraction        | 0.00947     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0783     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 465         |\n",
      "|    policy_gradient_loss | -0.00544    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0453      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 190   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 344   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030878078 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0588     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0444      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 358         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010557922 |\n",
      "|    clip_fraction        | 0.004       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0726     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0176      |\n",
      "|    n_updates            | 495         |\n",
      "|    policy_gradient_loss | -0.00479    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0458      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1218.35 +/- 2.55\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.22e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011625465 |\n",
      "|    clip_fraction        | 0.00482     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0892     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00259    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0459      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 191   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 373   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 380         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008745402 |\n",
      "|    clip_fraction        | 0.00283     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0709     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00515     |\n",
      "|    n_updates            | 525         |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0455      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=1223.67 +/- 2.05\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 1.22e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 75000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01994204 |\n",
      "|    clip_fraction        | 0.0174     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.93      |\n",
      "|    explained_variance   | -0.0732    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.00574   |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 0.0445     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 191   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 396   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018629372 |\n",
      "|    clip_fraction        | 0.0199      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | -0.0824     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00659     |\n",
      "|    n_updates            | 555         |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0454      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017247554 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | -0.052      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0447      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    tensorboard_log=\"./ppo_tensorboard_logs/DWF_v2/\",\n",
    "    learning_rate=1e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.98,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.4,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.8,               # boost critic learning slightly\n",
    "    max_grad_norm=0.5,\n",
    "    n_epochs=15,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=78000, callback=eval_callback)\n",
    "model.save(\"models/dwf_model_v2\")\n",
    "\n",
    "# Save normalization stats for later evaluation use\n",
    "train_env.save(\"models/dwf_vecnormalize.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")\n",
    "\n",
    "# Define RL Env\n",
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "        t_i = row['Request to Pickup']\n",
    "        delta = 0.2 * base_fare\n",
    "\n",
    "        # Behavior-based features\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "\n",
    "        cr_input_initial = 1.2 * rank_percentile - 1.3 * RPI\n",
    "        original_CR = 1.0 / (1.0 + np.exp(-cr_input_initial))\n",
    "\n",
    "        # Default: no intervention\n",
    "        fare_adjustment, rider_incentive = 0.0, 0.0\n",
    "\n",
    "        # Only apply RL action if original CR >= 0.5 \n",
    "        if original_CR >= 0.5:\n",
    "            fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "\n",
    "        # DPI based on actual action (whether applied or not)\n",
    "        action_signal = rider_incentive + abs(fare_adjustment * base_fare)\n",
    "        DPI = np.clip(1.0 - np.exp(-0.6 * action_signal) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # Recalculate CR using full action-aware formula\n",
    "        cr_input = 1.2 * rank_percentile - 1.3 * RPI - 2.0 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # Reward and Cost Logic\n",
    "        base_reward = 1.5 * (1.0 - CR)\n",
    "\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        cost_ratio = cost / (base_fare + 5)\n",
    "        cost_penalty = 0.5 * max(cost_ratio - 0.2, 0.0)\n",
    "        efficiency_bonus = 0.2 if ride_completed and cost_ratio < 0.2 else 0.0\n",
    "\n",
    "        reward = base_reward - cost_penalty + efficiency_bonus\n",
    "        reward = np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"RPI\": RPI,\n",
    "            \"DPI\": DPI,\n",
    "            \"cost\": cost,\n",
    "            \"base_reward\": base_reward,\n",
    "            \"cost_penalty\": cost_penalty,\n",
    "            \"original_CR\": original_CR\n",
    "        }\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "# Ensure logs directory exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Training env\n",
    "train_env = DummyVecEnv([lambda: RideHailingEnv_DWF(train_df)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_reward=10.0)\n",
    "\n",
    "# Validation env\n",
    "val_env = DummyVecEnv([lambda: RideHailingEnv_DWF(val_df)])\n",
    "val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# 9\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    tensorboard_log=\"./ppo_tensorboard_logs/DWF_v2/\",\n",
    "    learning_rate=1e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.98,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.4,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.8,               # boost critic learning slightly\n",
    "    max_grad_norm=0.5,\n",
    "    n_epochs=15,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=78000, callback=eval_callback)\n",
    "model.save(\"models/dwf_model_v2\")\n",
    "\n",
    "# Save normalization stats for later evaluation use\n",
    "train_env.save(\"models/dwf_vecnormalize.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
