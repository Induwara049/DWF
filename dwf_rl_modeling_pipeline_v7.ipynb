{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline: DWF Optimization with RL & Demand Forecasting\n",
    "This notebook implements a Deep Reinforcement Learning model to dynamically optimize fare incentives using preprocessed and enriched ride-hailing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "\n",
    "        # --- Behavior-based cancellation modeling\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "        delta = 0.2 * base_fare\n",
    "        DPI = np.clip((rider_incentive + fare_adjustment * base_fare) / (t_i + 1e-5) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        cr_input = 0.75 * rank_percentile - 1.1 * RPI - 0.9 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        reward = 4.0 if ride_completed else -4.0   # Increase absolute signal\n",
    "        cost_penalty = (rider_incentive + abs(fare_adjustment) * delta) / (base_fare + 5)\n",
    "        reward -= 1.8 * cost_penalty               # Reduce penalty scaling\n",
    "\n",
    "        if rider_incentive > 3.0 or abs(fare_adjustment) > 0.12:\n",
    "            reward -= 0.3                          # Softer soft-penalty\n",
    "\n",
    "        reward = np.clip(reward, -8.0, 6.0)        # Wider positive range\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "        return obs, reward, done, False, {\"CR\": CR}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RideHailingEnv_DWF(Env):\n",
    "#     def __init__(self, df):\n",
    "#         super(RideHailingEnv_DWF, self).__init__()\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "#         self.current_idx = 0\n",
    "#         self.episode_limit = 1000\n",
    "#         self.episode_start = 0\n",
    "\n",
    "#         self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "#         self.action_space = spaces.Box(\n",
    "#             low=np.array([-0.15, 0.0]),\n",
    "#             high=np.array([0.15, 5.0]),\n",
    "#             dtype=np.float32\n",
    "#         )\n",
    "\n",
    "#     def reset(self, *, seed=None, options=None):\n",
    "#         super().reset(seed=seed)\n",
    "#         self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "#         self.episode_start = self.current_idx\n",
    "#         return self._get_observation(), {}\n",
    "\n",
    "#     def step(self, action):\n",
    "#         if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "#             obs = self._get_observation()\n",
    "#             done = True\n",
    "#             reward = 0.0\n",
    "#             self.episode_start = self.current_idx + 1\n",
    "#             return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "#         row = self.df.iloc[self.current_idx]\n",
    "#         fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "#         t_i = row['Request to Pickup']\n",
    "#         base_fare = row.get('Base Fare', 10.0)\n",
    "#         delta = 0.2 * base_fare\n",
    "\n",
    "#         # --- Behavior modeling\n",
    "#         rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "#         epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "#         RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "#         DPI = np.clip((rider_incentive + fare_adjustment * base_fare) / (t_i + 1e-5) + epsilon, 0.0, 1.0)\n",
    "\n",
    "#         cr_input = 0.75 * rank_percentile - 1.1 * RPI - 0.9 * DPI\n",
    "#         CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "#         ride_completed = CR < 0.5\n",
    "\n",
    "#         # --- Final Reward (soft, scaled, efficient)\n",
    "#         base_reward = 1.0 - CR                             # Lower CR = better\n",
    "#         cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "#         cost_ratio = cost / (base_fare + 5)\n",
    "#         cost_penalty = 0.5 * cost_ratio                    # Soft cost penalty\n",
    "\n",
    "#         efficiency_bonus = 0.2 if (ride_completed and cost_ratio < 0.2) else 0.0\n",
    "#         overuse_penalty = 0.2 if rider_incentive > 3.0 or abs(fare_adjustment) > 0.15 else 0.0\n",
    "\n",
    "#         reward = base_reward - cost_penalty + efficiency_bonus - overuse_penalty\n",
    "#         reward = np.clip(reward, -1.5, 1.5)\n",
    "\n",
    "#         self.current_idx += 1\n",
    "#         return self._get_observation(), reward, False, False, {\"CR\": CR}\n",
    "\n",
    "\n",
    "\n",
    "#     def _get_observation(self):\n",
    "#         if self.current_idx >= len(self.df):\n",
    "#             return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "#         row = self.df.iloc[self.current_idx]\n",
    "#         return np.array([\n",
    "#             row['Pickup Location'],\n",
    "#             row['Request to Pickup'],\n",
    "#             row['Time of Day'],\n",
    "#             row['Month of Year'],\n",
    "#             row['RPI'],\n",
    "#             row['DPI'],\n",
    "#             row['CR'],\n",
    "#             row['Historical Demand Forecast']\n",
    "#         ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure logs directory exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Training env\n",
    "train_env = DummyVecEnv([lambda: RideHailingEnv_DWF(train_df)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_reward=5.0)\n",
    "\n",
    "# Validation env\n",
    "val_env = DummyVecEnv([lambda: RideHailingEnv_DWF(val_df)])\n",
    "val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard_logs/PPO_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 366  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 333         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015201694 |\n",
      "|    clip_fraction        | 0.00404     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0067      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.211       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.496       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=557.29 +/- 159.29\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 557         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014853435 |\n",
      "|    clip_fraction        | 0.00472     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0674      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.134       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.29        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 211  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 29   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021995593 |\n",
      "|    clip_fraction        | 0.0115      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0641      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0732      |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.00873    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=612.46 +/- 199.95\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 612         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011718168 |\n",
      "|    clip_fraction        | 0.00267     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0412      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 197   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021677377 |\n",
      "|    clip_fraction        | 0.00973     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0609      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022705935 |\n",
      "|    clip_fraction        | 0.0118      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0882      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0823      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00827    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=464.87 +/- 72.80\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 465         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017213147 |\n",
      "|    clip_fraction        | 0.00947     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0532      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0638      |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 201   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017298251 |\n",
      "|    clip_fraction        | 0.00947     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0671      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0761      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00787    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=502.50 +/- 200.00\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 502         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017032195 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0831      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 195   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 104   |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024286069 |\n",
      "|    clip_fraction        | 0.0202      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0798      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021789938 |\n",
      "|    clip_fraction        | 0.00804     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0849      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | -0.00939    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=443.24 +/- 166.18\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 443        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02378593 |\n",
      "|    clip_fraction        | 0.0138     |\n",
      "|    clip_range           | 0.5        |\n",
      "|    entropy_loss         | -2.86      |\n",
      "|    explained_variance   | 0.0827     |\n",
      "|    learning_rate        | 0.0002     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 197   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 134   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015737234 |\n",
      "|    clip_fraction        | 0.00527     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0845      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0651      |\n",
      "|    n_updates            | 195         |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=424.46 +/- 114.76\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 424         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022725299 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0824      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00859    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 196   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 156   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016069954 |\n",
      "|    clip_fraction        | 0.00898     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0776      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.00719    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020834256 |\n",
      "|    clip_fraction        | 0.0131      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0311      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00851    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=571.91 +/- 175.98\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 572         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023614418 |\n",
      "|    clip_fraction        | 0.0118      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 255         |\n",
      "|    policy_gradient_loss | -0.00862    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 200   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 183   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024535343 |\n",
      "|    clip_fraction        | 0.0138      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0911      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0731      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=759.34 +/- 104.38\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 759         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026937002 |\n",
      "|    clip_fraction        | 0.0158      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0736      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 285         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 198   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 206   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 213        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01590027 |\n",
      "|    clip_fraction        | 0.00426    |\n",
      "|    clip_range           | 0.5        |\n",
      "|    entropy_loss         | -2.83      |\n",
      "|    explained_variance   | 0.101      |\n",
      "|    learning_rate        | 0.0002     |\n",
      "|    loss                 | 0.0768     |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.00566   |\n",
      "|    std                  | 0.995      |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=545.09 +/- 215.94\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 545         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022130826 |\n",
      "|    clip_fraction        | 0.0149      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0941      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 315         |\n",
      "|    policy_gradient_loss | -0.00927    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 196   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 229   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029373873 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.0949      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0455      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024264116 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 345         |\n",
      "|    policy_gradient_loss | -0.00973    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=506.10 +/- 189.45\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 506         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013824074 |\n",
      "|    clip_fraction        | 0.00856     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0997      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0842      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 197   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 258   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029495368 |\n",
      "|    clip_fraction        | 0.0246      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.0757      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 375         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=582.75 +/- 169.05\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 583         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030278454 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0178      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 197   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 280   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017754968 |\n",
      "|    clip_fraction        | 0.00843     |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 405         |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 202       |\n",
      "|    iterations           | 29        |\n",
      "|    time_elapsed         | 293       |\n",
      "|    total_timesteps      | 59392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0281896 |\n",
      "|    clip_fraction        | 0.019     |\n",
      "|    clip_range           | 0.5       |\n",
      "|    entropy_loss         | -2.77     |\n",
      "|    explained_variance   | 0.102     |\n",
      "|    learning_rate        | 0.0002    |\n",
      "|    loss                 | 0.0947    |\n",
      "|    n_updates            | 420       |\n",
      "|    policy_gradient_loss | -0.0113   |\n",
      "|    std                  | 0.963     |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=522.99 +/- 130.74\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 523       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 60000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0305327 |\n",
      "|    clip_fraction        | 0.0277    |\n",
      "|    clip_range           | 0.5       |\n",
      "|    entropy_loss         | -2.75     |\n",
      "|    explained_variance   | 0.112     |\n",
      "|    learning_rate        | 0.0002    |\n",
      "|    loss                 | 0.0459    |\n",
      "|    n_updates            | 435       |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    std                  | 0.953     |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 199   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 308   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 315        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03557266 |\n",
      "|    clip_fraction        | 0.0307     |\n",
      "|    clip_range           | 0.5        |\n",
      "|    entropy_loss         | -2.74      |\n",
      "|    explained_variance   | 0.0976     |\n",
      "|    learning_rate        | 0.0002     |\n",
      "|    loss                 | 0.048      |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.952      |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=657.53 +/- 124.83\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 658         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026508117 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.0829      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 465         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 198   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 330   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024288872 |\n",
      "|    clip_fraction        | 0.0144      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.0932      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 345        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01776399 |\n",
      "|    clip_fraction        | 0.00853    |\n",
      "|    clip_range           | 0.5        |\n",
      "|    entropy_loss         | -2.71      |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.0002     |\n",
      "|    loss                 | 0.0366     |\n",
      "|    n_updates            | 495        |\n",
      "|    policy_gradient_loss | -0.00857   |\n",
      "|    std                  | 0.939      |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=408.44 +/- 223.55\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 408         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026704738 |\n",
      "|    clip_fraction        | 0.0282      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.0893      |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0748      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 198   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 360   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 367         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029612668 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0558      |\n",
      "|    n_updates            | 525         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=334.60 +/- 78.84\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 335         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026551336 |\n",
      "|    clip_fraction        | 0.0245      |\n",
      "|    clip_range           | 0.5         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.0619      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 198   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 382   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    tensorboard_log=\"./ppo_tensorboard_logs/\",\n",
    "    learning_rate=2e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.98,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.5,               # softer clipping\n",
    "    ent_coef=0.01,                # boost exploration\n",
    "    vf_coef=0.7,\n",
    "    max_grad_norm=0.5,            # prevent exploding grads\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    "    n_epochs=15,\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=75000, callback=eval_callback)\n",
    "model.save(\"models/dwf_model\")\n",
    "\n",
    "# Save normalization stats for later evaluation use\n",
    "train_env.save(\"models/dwf_vecnormalize.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
