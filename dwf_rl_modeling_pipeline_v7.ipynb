{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline: DWF Optimization with RL & Demand Forecasting\n",
    "This notebook implements a Deep Reinforcement Learning model to dynamically optimize fare incentives using preprocessed and enriched ride-hailing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "        delta = 0.2 * base_fare\n",
    "\n",
    "        # --- Behavior-based features\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # Amplify the impact of actions using exponential DPI\n",
    "        action_signal = rider_incentive + abs(fare_adjustment * base_fare)\n",
    "        DPI = np.clip(1.0 - np.exp(-0.6 * action_signal) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # CR: stronger dependency on DPI (action-driven cancellation)\n",
    "        cr_input = 1.2 * rank_percentile - 1.3 * RPI - 2.0 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # === Reward Components\n",
    "        base_reward = 1.5 * (1.0 - CR)             # Completion-focused reward\n",
    "\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        cost_ratio = cost / (base_fare + 5)\n",
    "        cost_penalty = 0.5 * max(cost_ratio - 0.2, 0.0)  # Penalty only after 20%\n",
    "\n",
    "        efficiency_bonus = 0.2 if ride_completed and cost_ratio < 0.2 else 0.0\n",
    "\n",
    "        reward = base_reward - cost_penalty + efficiency_bonus\n",
    "        reward = np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"RPI\": RPI,\n",
    "            \"DPI\": DPI,\n",
    "            \"cost\": cost,\n",
    "            \"base_reward\": base_reward,\n",
    "            \"cost_penalty\": cost_penalty\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure logs directory exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Training env\n",
    "train_env = DummyVecEnv([lambda: RideHailingEnv_DWF(train_df)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_reward=10.0)\n",
    "\n",
    "# Validation env\n",
    "val_env = DummyVecEnv([lambda: RideHailingEnv_DWF(val_df)])\n",
    "val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard_logs/DWF_v11/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 351  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 329         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005405333 |\n",
      "|    clip_fraction        | 0.000358    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.57       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.199       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.628       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=1880.22 +/- 18.25\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.88e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007157405 |\n",
      "|    clip_fraction        | 0.00176     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -1.33       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.277       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 217  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 28   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021157656 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -1.02        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00598      |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=1897.46 +/- 13.82\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.9e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006455784 |\n",
      "|    clip_fraction        | 0.00153     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.883      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00379    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0848      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009191811 |\n",
      "|    clip_fraction        | 0.00352     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.916      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00131    |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0583      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 225          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063612997 |\n",
      "|    clip_fraction        | 0.00173      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.629       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00981     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0483       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=1856.52 +/- 4.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.86e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002160362 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.542      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00795     |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 208   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037625174 |\n",
      "|    clip_fraction        | 0.000716     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.403       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00151      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.0459       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1848.60 +/- 12.06\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.85e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030138833 |\n",
      "|    clip_fraction        | 0.00013      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | -0.388       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00126     |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.0475       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 204   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 100   |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 107        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00735866 |\n",
      "|    clip_fraction        | 0.00166    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.9       |\n",
      "|    explained_variance   | -0.286     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0087    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.00507   |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 0.0477     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 114        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00337459 |\n",
      "|    clip_fraction        | 0.000163   |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.9       |\n",
      "|    explained_variance   | -0.296     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00809    |\n",
      "|    n_updates            | 165        |\n",
      "|    policy_gradient_loss | -0.00243   |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 0.0482     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=1897.55 +/- 9.00\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.9e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004590299 |\n",
      "|    clip_fraction        | 0.000586    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | -0.252      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0173      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 206   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 129   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002837079 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | -0.217      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0181      |\n",
      "|    n_updates            | 195         |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.0474      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1884.92 +/- 16.56\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.88e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057074632 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.202       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00209      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.0478       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 150   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048506605 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.236       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00517     |\n",
      "|    n_updates            | 225          |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.0483       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005736602 |\n",
      "|    clip_fraction        | 0.000846    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -0.181      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00718     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.0473      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=1878.32 +/- 6.21\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.88e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072250143 |\n",
      "|    clip_fraction        | 0.0029       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | -0.151       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0108       |\n",
      "|    n_updates            | 255          |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.0471       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 179   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013395129 |\n",
      "|    clip_fraction        | 0.00934     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.167      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00271     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0073     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0476      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1919.63 +/- 5.75\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 1.92e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01210433 |\n",
      "|    clip_fraction        | 0.00645    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.93      |\n",
      "|    explained_variance   | -0.144     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00479    |\n",
      "|    n_updates            | 285        |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 0.0468     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 200   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 207          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065283272 |\n",
      "|    clip_fraction        | 0.0013       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | -0.127       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00974     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.0464       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=1930.98 +/- 7.72\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.93e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013919903 |\n",
      "|    clip_fraction        | 0.0084      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.137      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 315         |\n",
      "|    policy_gradient_loss | -0.00848    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0471      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 201   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 223   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015036022 |\n",
      "|    clip_fraction        | 0.00905     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.0723     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0655      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007694769 |\n",
      "|    clip_fraction        | 0.00384     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | -0.0892     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00825     |\n",
      "|    n_updates            | 345         |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0458      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1957.62 +/- 6.76\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.96e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011286126 |\n",
      "|    clip_fraction        | 0.00651     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | -0.102      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0453      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 202   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 252   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 259        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01199043 |\n",
      "|    clip_fraction        | 0.00547    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.94      |\n",
      "|    explained_variance   | -0.0722    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.019     |\n",
      "|    n_updates            | 375        |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 0.0443     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=1964.65 +/- 3.38\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.96e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013012608 |\n",
      "|    clip_fraction        | 0.00732     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.95       |\n",
      "|    explained_variance   | -0.106      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0462      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 201   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 274   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 281          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066496106 |\n",
      "|    clip_fraction        | 0.000911     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | -0.0889      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.000779    |\n",
      "|    n_updates            | 405          |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.046        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 288         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008286763 |\n",
      "|    clip_fraction        | 0.00225     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | -0.0968     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0459      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1955.92 +/- 4.05\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.96e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067570442 |\n",
      "|    clip_fraction        | 0.0015       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | -0.0819      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00452     |\n",
      "|    n_updates            | 435          |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 0.045        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 202   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 303   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006899586 |\n",
      "|    clip_fraction        | 0.00143     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.95       |\n",
      "|    explained_variance   | -0.0894     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0451      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=1966.20 +/- 6.75\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.97e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009662564 |\n",
      "|    clip_fraction        | 0.00299     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.96       |\n",
      "|    explained_variance   | -0.0995     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 465         |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 200   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 326   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007182926 |\n",
      "|    clip_fraction        | 0.0013      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.96       |\n",
      "|    explained_variance   | -0.0818     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00193     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00515    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0451      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023223773 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.96       |\n",
      "|    explained_variance   | -0.0496     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 495         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.045       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1969.13 +/- 2.75\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.97e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144332405 |\n",
      "|    clip_fraction        | 0.00628      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.96        |\n",
      "|    explained_variance   | -0.0996      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.0207      |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00625     |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 0.045        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 202   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 354   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011987457 |\n",
      "|    clip_fraction        | 0.00726     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.97       |\n",
      "|    explained_variance   | -0.0826     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 525         |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.045       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=1969.63 +/- 3.85\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 1.97e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 75000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070129903 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.97        |\n",
      "|    explained_variance   | -0.0758      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.0135      |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 0.0447       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 201   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 376   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 203       |\n",
      "|    iterations           | 38        |\n",
      "|    time_elapsed         | 383       |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0102447 |\n",
      "|    clip_fraction        | 0.00309   |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -2.97     |\n",
      "|    explained_variance   | -0.0582   |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00982  |\n",
      "|    n_updates            | 555       |\n",
      "|    policy_gradient_loss | -0.00612  |\n",
      "|    std                  | 1.07      |\n",
      "|    value_loss           | 0.0456    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023054514 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.96       |\n",
      "|    explained_variance   | -0.0682     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00944     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0447      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 9\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    tensorboard_log=\"./ppo_tensorboard_logs/DWF/\",\n",
    "    learning_rate=1e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.98,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.4,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.8,               # boost critic learning slightly\n",
    "    max_grad_norm=0.5,\n",
    "    n_epochs=15,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=78000, callback=eval_callback)\n",
    "model.save(\"models/dwf_model\")\n",
    "\n",
    "# Save normalization stats for later evaluation use\n",
    "train_env.save(\"models/dwf_vecnormalize.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
