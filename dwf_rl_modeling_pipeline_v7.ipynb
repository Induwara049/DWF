{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline: DWF Optimization with RL & Demand Forecasting\n",
    "This notebook implements a Deep Reinforcement Learning model to dynamically optimize fare incentives using preprocessed and enriched ride-hailing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "        delta = 0.2 * base_fare\n",
    "\n",
    "        # --- Behavior-based features\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # Amplify the impact of actions using exponential DPI\n",
    "        action_signal = rider_incentive + abs(fare_adjustment * base_fare)\n",
    "        DPI = np.clip(1.0 - np.exp(-0.6 * action_signal) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # CR: stronger dependency on DPI (action-driven cancellation)\n",
    "        cr_input = 1.2 * rank_percentile - 1.3 * RPI - 2.0 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # === Reward Components\n",
    "        base_reward = 1.5 * (1.0 - CR)             # Completion-focused reward\n",
    "\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        cost_ratio = cost / (base_fare + 5)\n",
    "        cost_penalty = 0.5 * max(cost_ratio - 0.2, 0.0)  # Penalty only after 20%\n",
    "\n",
    "        efficiency_bonus = 0.2 if ride_completed and cost_ratio < 0.2 else 0.0\n",
    "\n",
    "        reward = base_reward - cost_penalty + efficiency_bonus\n",
    "        reward = np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"RPI\": RPI,\n",
    "            \"DPI\": DPI,\n",
    "            \"cost\": cost,\n",
    "            \"base_reward\": base_reward,\n",
    "            \"cost_penalty\": cost_penalty\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure logs directory exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Training env\n",
    "train_env = DummyVecEnv([lambda: RideHailingEnv_DWF(train_df)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_reward=10.0)\n",
    "\n",
    "# Validation env\n",
    "val_env = DummyVecEnv([lambda: RideHailingEnv_DWF(val_df)])\n",
    "val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 9\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    tensorboard_log=\"./ppo_tensorboard_logs/DWF/\",\n",
    "    learning_rate=1e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.98,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.4,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.8,               # boost critic learning slightly\n",
    "    max_grad_norm=0.5,\n",
    "    n_epochs=15,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=78000, callback=eval_callback)\n",
    "model.save(\"models/dwf_model\")\n",
    "\n",
    "# Save normalization stats for later evaluation use\n",
    "train_env.save(\"models/dwf_vecnormalize.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
