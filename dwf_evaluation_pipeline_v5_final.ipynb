{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Evaluation Pipeline: DWF RL Model\n",
    "This notebook evaluates the trained RL modelâ€™s performance using core metrics like cancellation rate, rider wait time, and profit impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "        delta = 0.2 * base_fare\n",
    "\n",
    "        # --- Behavior-based features\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # Amplify the impact of actions using exponential DPI\n",
    "        action_signal = rider_incentive + abs(fare_adjustment * base_fare)\n",
    "        DPI = np.clip(1.0 - np.exp(-0.6 * action_signal) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # CR: stronger dependency on DPI (action-driven cancellation)\n",
    "        cr_input = 1.2 * rank_percentile - 1.3 * RPI - 2.0 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # === Reward Components\n",
    "        base_reward = 1.5 * (1.0 - CR)             # Completion-focused reward\n",
    "\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        cost_ratio = cost / (base_fare + 5)\n",
    "        cost_penalty = 0.5 * max(cost_ratio - 0.2, 0.0)  # Penalty only after 20%\n",
    "\n",
    "        efficiency_bonus = 0.2 if ride_completed and cost_ratio < 0.2 else 0.0\n",
    "\n",
    "        reward = base_reward - cost_penalty + efficiency_bonus\n",
    "        reward = np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"RPI\": RPI,\n",
    "            \"DPI\": DPI,\n",
    "            \"cost\": cost,\n",
    "            \"base_reward\": base_reward,\n",
    "            \"cost_penalty\": cost_penalty\n",
    "        }\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Perfoemance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load logs\n",
    "ea = event_accumulator.EventAccumulator('ppo_tensorboard_logs/DWF/PPO_1')\n",
    "ea.Reload()\n",
    "\n",
    "# Extract scalars\n",
    "val_loss = ea.Scalars('train/value_loss')\n",
    "pol_loss = ea.Scalars('train/policy_gradient_loss')\n",
    "entropy  = ea.Scalars('train/entropy_loss')\n",
    "eval_rew = ea.Scalars('eval/mean_reward')\n",
    "\n",
    "# Convert to lists\n",
    "val_steps, val_vals = zip(*[(x.step, x.value) for x in val_loss])\n",
    "pol_steps, pol_vals = zip(*[(x.step, x.value) for x in pol_loss])\n",
    "ent_steps, ent_vals = zip(*[(x.step, x.value) for x in entropy])\n",
    "eval_steps, eval_vals = zip(*[(x.step, x.value) for x in eval_rew])\n",
    "\n",
    "# === Plot 1: Value Loss ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_steps, val_vals, label=\"Value Loss\", color='tab:blue')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Value Loss\")\n",
    "plt.title(\"PPO Training: Value Loss Over Time\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: Policy Gradient Loss ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pol_steps, pol_vals, label=\"Policy Gradient Loss\", color='tab:orange')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Policy Loss\")\n",
    "plt.title(\"PPO Training: Policy Gradient Loss Over Time\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 3: Entropy Loss ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ent_steps, ent_vals, label=\"Entropy Loss\", color='tab:green')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Entropy Loss\")\n",
    "plt.title(\"PPO Training: Entropy Loss Over Time\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 4: Evaluation Reward ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(eval_steps, eval_vals, label=\"Evaluation Reward Per Episode\", color='tab:red', linestyle='--')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Eval Reward\")\n",
    "plt.title(\"PPO Training: Evaluation Reward Over Time\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Framework Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = PPO.load(\"models/dwf_model\")\n",
    "test_df = pd.read_csv(\"datasets/test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model: PPO, test_df: pd.DataFrame):\n",
    "\n",
    "    print(f\"Model Name: {model}\")\n",
    "\n",
    "    env = RideHailingEnv_DWF(test_df.copy())  # use a copy to avoid modifying original\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_cost = 0\n",
    "    retained_from_cancel = 0\n",
    "    completed_rides = 0\n",
    "    original_cancellations = 0\n",
    "    ride_count = 0\n",
    "    incentive_total = 0\n",
    "    fare_adj_total = 0\n",
    "\n",
    "    while not done:\n",
    "        row = env.df.iloc[env.current_idx]\n",
    "        base_fare = row['Base Fare']\n",
    "        t_i = row['Request to Pickup']\n",
    "        original_cr = row['CR']  # original cancellation rate before action\n",
    "\n",
    "        # Count originally cancelled rides\n",
    "        originally_cancelled = original_cr >= 0.5\n",
    "        if originally_cancelled:\n",
    "            original_cancellations += 1\n",
    "\n",
    "        # Predict action\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        fare_adj = np.clip(action[0], -0.15, 0.15)\n",
    "        incentive = np.clip(action[1], 0.0, 5.0)\n",
    "\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        updated_cr = info[\"CR\"]\n",
    "\n",
    "        # Track completion & intervention success\n",
    "        ride_completed = updated_cr < 0.5\n",
    "        if ride_completed:\n",
    "            completed_rides += 1\n",
    "        if originally_cancelled and ride_completed:\n",
    "            retained_from_cancel += 1\n",
    "\n",
    "        # Track costs\n",
    "        delta = 0.2 * base_fare\n",
    "        cost = incentive + abs(fare_adj) * delta\n",
    "        total_cost += cost\n",
    "        incentive_total += incentive\n",
    "        fare_adj_total += abs(fare_adj)\n",
    "        ride_count += 1\n",
    "\n",
    "    # Metrics\n",
    "    avg_cancel_rate = 1 - (completed_rides / ride_count)\n",
    "    avg_incentive = incentive_total / ride_count\n",
    "    avg_fare_adj = fare_adj_total / ride_count\n",
    "    avg_cost_per_retained = total_cost / retained_from_cancel if retained_from_cancel else 0\n",
    "\n",
    "    print(\"\\nFinal Evaluation Results\")\n",
    "    print(f\"Avg Cancellation Rate: {avg_cancel_rate:.3f}\")\n",
    "    print(f\"Avg Incentive: ${avg_incentive:.2f}\")\n",
    "    print(f\"Avg Fare Adj: {avg_fare_adj:.3f}\")\n",
    "    print(f\"Avg Cost per Retained Ride: ${avg_cost_per_retained:.2f}\")\n",
    "    print(f\"Retained from Cancellation: {retained_from_cancel} out of {original_cancellations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = PPO.load(\"models/dwf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_on_test(test_model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for version in range(2, 8):  # v2 to v7\n",
    "#     model_path = f\"models/dwf_model_v{version}.zip\"\n",
    "#     print(f\"\\n Evaluating: Version{version}\")\n",
    "#     test_model = PPO.load(model_path)\n",
    "\n",
    "#     evaluate_model_on_test(test_model, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
