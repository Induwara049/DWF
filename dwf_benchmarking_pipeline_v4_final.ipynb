{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Pipeline: Compare Baseline vs DWF Model\n",
    "This notebook compares a fixed-incentive strategy against the DWF RL model using cancellation rate, incentives, and profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "        delta = 0.2 * base_fare\n",
    "\n",
    "        # --- Behavior-based features\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # Amplify the impact of actions using exponential DPI\n",
    "        action_signal = rider_incentive + abs(fare_adjustment * base_fare)\n",
    "        DPI = np.clip(1.0 - np.exp(-0.6 * action_signal) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        # CR: stronger dependency on DPI (action-driven cancellation)\n",
    "        cr_input = 1.2 * rank_percentile - 1.3 * RPI - 2.0 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # === Reward Components\n",
    "        base_reward = 1.5 * (1.0 - CR)             # Completion-focused reward\n",
    "\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        cost_ratio = cost / (base_fare + 5)\n",
    "        cost_penalty = 0.5 * max(cost_ratio - 0.2, 0.0)  # Penalty only after 20%\n",
    "\n",
    "        efficiency_bonus = 0.2 if ride_completed and cost_ratio < 0.2 else 0.0\n",
    "\n",
    "        reward = base_reward - cost_penalty + efficiency_bonus\n",
    "        reward = np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"RPI\": RPI,\n",
    "            \"DPI\": DPI,\n",
    "            \"cost\": cost,\n",
    "            \"base_reward\": base_reward,\n",
    "            \"cost_penalty\": cost_penalty\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1 Model\n",
    "class RideHailingEnv_Feature_Baseline(RideHailingEnv_DWF):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "\n",
    "# Baselie_2 Model\n",
    "class RideHailingEnv_Cost_Baseline(RideHailingEnv_DWF):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "\n",
    "        # === Basic ride cancellation model without synthetic features ===\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "\n",
    "        # Simplified CR estimation: no RPI/DPI, only wait time percentile\n",
    "        cr_input = 1.2 * rank_percentile + epsilon\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        # === Reward: purely focused on maximizing ride completions ===\n",
    "        reward = 1.0 if ride_completed else -1.0\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, done, False, {\n",
    "            \"CR\": CR,\n",
    "            \"cost\": 0.0,\n",
    "            \"base_reward\": reward,\n",
    "            \"cost_penalty\": 0.0,\n",
    "            \"RPI\": None,\n",
    "            \"DPI\": None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variant(env_cls, train_df, val_df, model_name):\n",
    "    # Train environment\n",
    "    train_env = DummyVecEnv([lambda: env_cls(train_df)])\n",
    "    train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "    # Validation environment\n",
    "    val_env = DummyVecEnv([lambda: env_cls(val_df)])\n",
    "    val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "    # Evaluation callback\n",
    "    eval_callback = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path=f\"./logs/{model_name}/\",\n",
    "        log_path=f\"./logs/{model_name}/\",\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        tensorboard_log=f\"./ppo_tensorboard_logs/{model_name}/\",\n",
    "        learning_rate=1e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=256,\n",
    "        gamma=0.98,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.4,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.8,               # boost critic learning slightly\n",
    "        max_grad_norm=0.5,\n",
    "        n_epochs=15,\n",
    "        policy_kwargs=dict(net_arch=[64, 64]),\n",
    "        verbose=1,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=78000, callback=eval_callback)\n",
    "\n",
    "    model.save(f\"models/{model_name}\")\n",
    "    train_env.save(f\"models/{model_name}_vecnormalize.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_variant(RideHailingEnv_Feature_Baseline, train_df, val_df, \"model_feature_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_variant(RideHailingEnv_Cost_Baseline, train_df, val_df, \"model_cost_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "\n",
    "def compute_score(avg_cost, avg_cr, alpha=0.6, beta=0.4):\n",
    "    return alpha * avg_cost + beta * avg_cr\n",
    "\n",
    "def evaluate_model(model_path, vecnormalize_path, env_class, test_df):\n",
    "    env = DummyVecEnv([lambda: Monitor(env_class(test_df))])\n",
    "    vec_norm = VecNormalize.load(vecnormalize_path, env)\n",
    "    vec_norm.training = False\n",
    "    vec_norm.norm_reward = False\n",
    "\n",
    "    model = PPO.load(model_path)\n",
    "\n",
    "    obs = vec_norm.reset()\n",
    "    total_cost = 0\n",
    "    total_revenue = 0\n",
    "    completed_rides = 0\n",
    "    total_cr = 0\n",
    "    num_steps = 0\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_norm.step(action)\n",
    "        info = info[0]\n",
    "\n",
    "        cr = info.get(\"CR\", 0.5)\n",
    "        total_cr += cr\n",
    "        is_completed = cr < 0.5\n",
    "        if is_completed:\n",
    "            completed_rides += 1\n",
    "\n",
    "        fare_adjustment, rider_incentive = np.round(action[0], 2)\n",
    "        base_fare = test_df.iloc[num_steps].get(\"Base Fare\", 10.0)\n",
    "        delta = 0.2 * base_fare\n",
    "        fare = base_fare + fare_adjustment * base_fare\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "\n",
    "        total_cost += cost\n",
    "\n",
    "        num_steps += 1\n",
    "        if num_steps >= len(test_df) or done:\n",
    "            break\n",
    "\n",
    "    avg_cr = total_cr / num_steps\n",
    "    avg_cost = total_cost / num_steps\n",
    "    score = compute_score(avg_cost, avg_cr)\n",
    "\n",
    "    return {\n",
    "        \"avg_cost\": avg_cost,\n",
    "        \"avg_cr\": avg_cr,\n",
    "        \"score\": score,\n",
    "        \"total_steps\": num_steps\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"datasets/test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "model_variants = {\n",
    "    \"Feature Base\": (\n",
    "        \"models/model_feature_base.zip\",\n",
    "        \"models/model_feature_base_vecnormalize.pkl\",\n",
    "        RideHailingEnv_Feature_Baseline\n",
    "    ),\n",
    "    \"DWF\": (\n",
    "        \"models/dwf_model.zip\",\n",
    "        \"models/dwf_vecnormalize.pkl\",\n",
    "        RideHailingEnv_DWF\n",
    "    ),\n",
    "    \"Cost Base\": (\n",
    "        \"models/model_cost_base.zip\",\n",
    "        \"models/model_cost_base_vecnormalize.pkl\",\n",
    "        RideHailingEnv_Cost_Baseline\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, (model_path, vecnorm_path, env_class) in model_variants.items():\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    results[name] = evaluate_model(model_path, vecnorm_path, env_class, test_df)\n",
    "    print(f\" â†’ Completed {results[name]['total_steps']} steps | \"\n",
    "          f\"CR: {results[name]['avg_cr']:.3f} | \"\n",
    "          f\"Cost: {results[name]['avg_cost']:.3f} | \"\n",
    "          f\"Revenue: {results[name]['avg_revenue']:.3f} | \"\n",
    "          f\"Score: {results[name]['score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(results.keys())\n",
    "avg_costs = [results[k][\"avg_cost\"] for k in labels]\n",
    "completion_rates = [results[k][\"completion_rate\"] for k in labels]\n",
    "cancel_rates = [results[k][\"avg_cr\"] for k in labels]\n",
    "avg_revenues = [results[k][\"avg_revenue\"] for k in labels]\n",
    "scores = [results[k][\"score\"] for k in labels]\n",
    "\n",
    "# Plot 1: Avg Incentive Cost per Ride\n",
    "plt.figure()\n",
    "plt.bar(labels, avg_costs)\n",
    "plt.ylabel(\"Avg Cost per Ride\")\n",
    "plt.title(\"Model Comparison: Incentive Cost\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Average Cancellation Rate (CR)\n",
    "plt.figure()\n",
    "plt.bar(labels, cancel_rates)\n",
    "plt.ylabel(\"Avg Cancellation Rate (CR)\")\n",
    "plt.title(\"Model Comparison: Cancellation Rate\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 5: Custom Composite Score (Lower = Better)\n",
    "plt.figure()\n",
    "plt.bar(labels, scores)\n",
    "plt.ylabel(\"Composite Score (Cost + CR)\")\n",
    "plt.title(\"Model Comparison: Cost-CR Score (Lower = Better)\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
