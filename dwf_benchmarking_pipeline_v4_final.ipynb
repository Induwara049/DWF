{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Pipeline: Compare Baseline vs DWF Model\n",
    "This notebook compares a fixed-incentive strategy against the DWF RL model using cancellation rate, incentives, and profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from scipy.special import expit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/train_split.csv\")\n",
    "val_df =  pd.read_csv(\"datasets/val_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RideHailingEnv_DWF(Env):\n",
    "    def __init__(self, df):\n",
    "        super(RideHailingEnv_DWF, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_idx = 0\n",
    "        self.episode_limit = 1000\n",
    "        self.episode_start = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-0.15, 0.0]),\n",
    "            high=np.array([0.15, 5.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_idx = np.random.randint(0, len(self.df) - self.episode_limit)\n",
    "        self.episode_start = self.current_idx\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_idx >= len(self.df) - 1 or (self.current_idx - self.episode_start >= self.episode_limit):\n",
    "            obs = self._get_observation()\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "            self.episode_start = self.current_idx + 1\n",
    "            return obs, reward, done, False, {\"CR\": 0.5}\n",
    "\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        fare_adjustment, rider_incentive = np.round(action, 2)\n",
    "        t_i = row['Request to Pickup']\n",
    "        base_fare = row.get('Base Fare', 10.0)\n",
    "\n",
    "        # --- Behavior-based cancellation modeling\n",
    "        rank_percentile = (self.df['Request to Pickup'] < t_i).sum() / len(self.df)\n",
    "        epsilon = np.random.normal(loc=0.0, scale=0.02)\n",
    "\n",
    "        RPI = np.clip(1.0 - rank_percentile + epsilon, 0.0, 1.0)\n",
    "        delta = 0.2 * base_fare\n",
    "        DPI = np.clip((rider_incentive + fare_adjustment * base_fare) / (t_i + 1e-5) + epsilon, 0.0, 1.0)\n",
    "\n",
    "        cr_input = 0.75 * rank_percentile - 1.1 * RPI - 0.9 * DPI\n",
    "        CR = 1.0 / (1.0 + np.exp(-cr_input))\n",
    "        ride_completed = CR < 0.5\n",
    "\n",
    "        reward = 4.0 if ride_completed else -4.0   # Increase absolute signal\n",
    "        cost_penalty = (rider_incentive + abs(fare_adjustment) * delta) / (base_fare + 5)\n",
    "        reward -= 0.8 * cost_penalty               # Reduce penalty scaling\n",
    "\n",
    "        if rider_incentive > 3.0 or abs(fare_adjustment) > 0.12:\n",
    "            reward -= 0.3                          # Softer soft-penalty\n",
    "\n",
    "        reward = np.clip(reward, -8.0, 6.0)        # Wider positive range\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = False\n",
    "        obs = self._get_observation()\n",
    "        return obs, reward, done, False, {\"CR\": CR}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_idx >= len(self.df):\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI'],\n",
    "            row['CR'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "class RideHailingEnv_Base(RideHailingEnv_DWF):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year']\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "\n",
    "# RPI Model\n",
    "class RideHailingEnv_RPI(RideHailingEnv_DWF):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI']\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "# RPI_DPI Model\n",
    "class RideHailingEnv_RPI_DPI(RideHailingEnv_DWF):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(6,), dtype=np.float32)\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['RPI'],\n",
    "            row['DPI']\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "# HDF Model\n",
    "class RideHailingEnv_HDF(RideHailingEnv_DWF):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        row = self.df.iloc[self.current_idx]\n",
    "        return np.array([\n",
    "            row['Pickup Location'],\n",
    "            row['Request to Pickup'],\n",
    "            row['Time of Day'],\n",
    "            row['Month of Year'],\n",
    "            row['Historical Demand Forecast']\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variant(env_cls, train_df, val_df, model_name):\n",
    "    # Train environment\n",
    "    train_env = DummyVecEnv([lambda: env_cls(train_df)])\n",
    "    train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "    # Validation environment\n",
    "    val_env = DummyVecEnv([lambda: env_cls(val_df)])\n",
    "    val_env = VecNormalize(val_env, training=False, norm_obs=True, norm_reward=False)\n",
    "\n",
    "    # Evaluation callback\n",
    "    eval_callback = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path=f\"./logs/{model_name}/\",\n",
    "        log_path=f\"./logs/{model_name}/\",\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        tensorboard_log=f\"./ppo_tensorboard_logs/{model_name}/\",\n",
    "        learning_rate=4e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=256,\n",
    "        gamma=0.98,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.4,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.4,\n",
    "        max_grad_norm=0.5,\n",
    "        policy_kwargs=dict(net_arch=[64, 64]),\n",
    "        verbose=1,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=200000, callback=eval_callback)\n",
    "    model.save(f\"models/{model_name}\")\n",
    "    train_env.save(f\"models/{model_name}_vecnormalize.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard_logs/model_base/PPO_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 363  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017746821 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00636    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.313       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=575.58 +/- 173.67\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 576         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013516211 |\n",
      "|    clip_fraction        | 0.00981     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0517      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 228  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023891449 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0581      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00809     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00943    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=558.57 +/- 212.24\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 559         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014377214 |\n",
      "|    clip_fraction        | 0.0171      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0567      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0421      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 222         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018167147 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0396      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013841883 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.06        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00451    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=435.83 +/- 85.66\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 436         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016541008 |\n",
      "|    clip_fraction        | 0.0182      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0598      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00293     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 215   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013895967 |\n",
      "|    clip_fraction        | 0.0152      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0889      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=468.69 +/- 191.30\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 469          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0150834825 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.088        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.0197       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00557     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.15         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019728731 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0784      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0427      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015666801 |\n",
      "|    clip_fraction        | 0.0235      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=427.57 +/- 81.82\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 428         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018466357 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0884      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00878    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 214   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021927549 |\n",
      "|    clip_fraction        | 0.041       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0844      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=520.63 +/- 190.11\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 521        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01637956 |\n",
      "|    clip_fraction        | 0.0184     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.81      |\n",
      "|    explained_variance   | 0.0929     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0244     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.00588   |\n",
      "|    std                  | 0.981      |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 145   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 216       |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 151       |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0182858 |\n",
      "|    clip_fraction        | 0.0288    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -2.81     |\n",
      "|    explained_variance   | 0.091     |\n",
      "|    learning_rate        | 0.0004    |\n",
      "|    loss                 | 0.0249    |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.00933  |\n",
      "|    std                  | 0.988     |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018613536 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0503      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=380.95 +/- 29.76\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 381         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014552565 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0685      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 213   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 172   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014136057 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0998      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00616     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=380.18 +/- 64.05\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 380         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021417733 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 193   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018028691 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.0955      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=422.75 +/- 189.87\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 423         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020176385 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.0917      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 210   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 214   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014627589 |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0894      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00409    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015613115 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00888    |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=668.34 +/- 207.69\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 668         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015549122 |\n",
      "|    clip_fraction        | 0.0202      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0898      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0256      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    std                  | 0.974       |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 242   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013076849 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.097       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=590.87 +/- 161.82\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 591         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020232467 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.0963      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 210   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 262   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 269        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01650727 |\n",
      "|    clip_fraction        | 0.0312     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.82      |\n",
      "|    explained_variance   | 0.0903     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0174     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0083    |\n",
      "|    std                  | 0.988      |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018105257 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=488.84 +/- 133.10\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 489         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018083688 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0955      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0051      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 290   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019873086 |\n",
      "|    clip_fraction        | 0.0245      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00328     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=525.92 +/- 168.82\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 526         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012894831 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00515    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 210   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 311   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018771373 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020721486 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0949      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00928     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=370.65 +/- 65.58\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014369259 |\n",
      "|    clip_fraction        | 0.0152      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0977      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00104    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00573    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 339   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014882055 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00442     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00686    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=578.23 +/- 149.02\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 578         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016731195 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00957    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 210   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 360   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017170792 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013822111 |\n",
      "|    clip_fraction        | 0.0192      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00549    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=538.71 +/- 210.11\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 539         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016968459 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 387   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 393         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012261512 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=567.79 +/- 204.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 568         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015469492 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0288      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 210   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 408   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 414          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098987175 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.00819      |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00504     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=348.90 +/- 46.24\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 349         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022912398 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.000829    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 210   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 428   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014412291 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0992      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00658    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 441         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013114534 |\n",
      "|    clip_fraction        | 0.0114      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0138      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=485.81 +/- 227.55\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 486         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014492995 |\n",
      "|    clip_fraction        | 0.0116      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0056     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 455   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 462        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01623604 |\n",
      "|    clip_fraction        | 0.0177     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.83      |\n",
      "|    explained_variance   | 0.0967     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.00503   |\n",
      "|    std                  | 0.996      |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=439.92 +/- 172.92\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 440         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014143886 |\n",
      "|    clip_fraction        | 0.0211      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 476    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 212       |\n",
      "|    iterations           | 50        |\n",
      "|    time_elapsed         | 482       |\n",
      "|    total_timesteps      | 102400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0224821 |\n",
      "|    clip_fraction        | 0.0426    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -2.81     |\n",
      "|    explained_variance   | 0.114     |\n",
      "|    learning_rate        | 0.0004    |\n",
      "|    loss                 | 0.00755   |\n",
      "|    n_updates            | 490       |\n",
      "|    policy_gradient_loss | -0.0116   |\n",
      "|    std                  | 0.989     |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 489         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021218177 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=559.11 +/- 199.50\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 559         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021846928 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0265      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 503    |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021832904 |\n",
      "|    clip_fraction        | 0.0328      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0833      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    std                  | 0.974       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=463.06 +/- 166.95\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 463         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014119927 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00616     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 524    |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 531         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018842928 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00291     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 537         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013258664 |\n",
      "|    clip_fraction        | 0.0148      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=475.85 +/- 146.16\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 476         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023610644 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 552    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016712854 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.099       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00876    |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=655.96 +/- 202.96\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 656         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021526797 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 573    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 580        |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01588773 |\n",
      "|    clip_fraction        | 0.0209     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.77      |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.00689   |\n",
      "|    std                  | 0.967      |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018121667 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0979      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0117      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0081     |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=517.92 +/- 149.21\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 518         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023678606 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 600    |\n",
      "|    total_timesteps | 126976 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014635734 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0272      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=616.28 +/- 199.69\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 616         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017085763 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00992     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 621    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 628         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017069142 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0964      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000693   |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=584.06 +/- 236.55\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 584         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023292493 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0997      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000777   |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 642    |\n",
      "|    total_timesteps | 135168 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 648         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026914807 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 655         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021123268 |\n",
      "|    clip_fraction        | 0.0377      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00836    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=457.10 +/- 177.94\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015584888 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0173      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 669    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019247033 |\n",
      "|    clip_fraction        | 0.0145      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=470.53 +/- 224.63\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 471         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015952768 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00931    |\n",
      "|    std                  | 0.938       |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 690    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 697        |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01325307 |\n",
      "|    clip_fraction        | 0.0118     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.71      |\n",
      "|    explained_variance   | 0.109      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.000838   |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.00565   |\n",
      "|    std                  | 0.939      |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 703         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016790763 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00039    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0069     |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=550.24 +/- 212.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 550         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016137503 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0225      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 717    |\n",
      "|    total_timesteps | 151552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 724         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012991451 |\n",
      "|    clip_fraction        | 0.0138      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=514.13 +/- 190.25\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 514         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 155000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020824451 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00919    |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 738    |\n",
      "|    total_timesteps | 155648 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 745         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014752535 |\n",
      "|    clip_fraction        | 0.018       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00839     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 751         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016260708 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.0981      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00127    |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    std                  | 0.934       |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=677.46 +/- 229.32\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 677         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021322321 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.0999      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.01        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 766    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 772         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019819673 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00789     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    std                  | 0.93        |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=513.11 +/- 186.42\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 513         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 165000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015969206 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    std                  | 0.932       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 787    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 794         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019290179 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0139      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 800        |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02412685 |\n",
      "|    clip_fraction        | 0.0497     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.66      |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00887    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.92       |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=371.07 +/- 142.95\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015323909 |\n",
      "|    clip_fraction        | 0.0193      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.915       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 815    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 821         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016948516 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00608     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.911       |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=469.52 +/- 206.25\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 175000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021403719 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00904    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.912       |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 836    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 842         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013317343 |\n",
      "|    clip_fraction        | 0.015       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.0987      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0293      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    std                  | 0.912       |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=518.73 +/- 243.49\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 519         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014931826 |\n",
      "|    clip_fraction        | 0.0185      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00696    |\n",
      "|    std                  | 0.908       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 857    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 864         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020189065 |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0309      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015527524 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00801     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=366.33 +/- 75.78\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 366         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019080382 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.0954      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000214   |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 885    |\n",
      "|    total_timesteps | 186368 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 891         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021292832 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    std                  | 0.901       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=556.07 +/- 223.36\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 556         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013327818 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    std                  | 0.889       |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 906    |\n",
      "|    total_timesteps | 190464 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 912         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015564347 |\n",
      "|    clip_fraction        | 0.0168      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00866     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00689    |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 919         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019712709 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00949     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.888       |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=505.21 +/- 206.90\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 505         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016300194 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    std                  | 0.888       |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 934    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 941         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017265474 |\n",
      "|    clip_fraction        | 0.0231      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | 0.0974      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    std                  | 0.881       |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=352.51 +/- 119.96\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 353         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021304943 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 209    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 956    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_variant(RideHailingEnv_Base, train_df, val_df, \"model_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard_logs/model_rpi/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 360  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 335        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01755374 |\n",
      "|    clip_fraction        | 0.0227     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.84      |\n",
      "|    explained_variance   | -0.352     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.056      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0081    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.513      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=738.60 +/- 269.61\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 739         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014958007 |\n",
      "|    clip_fraction        | 0.0132      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0646      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 227  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 27   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022120396 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0759      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00951    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=936.16 +/- 212.62\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 936         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007873093 |\n",
      "|    clip_fraction        | 0.00386     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0942      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 216   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017132409 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0691      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009124286 |\n",
      "|    clip_fraction        | 0.00269     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0807      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.03        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=716.58 +/- 182.30\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 717         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015268274 |\n",
      "|    clip_fraction        | 0.0149      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0746      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 223   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022990128 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0818      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=610.98 +/- 242.53\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 611         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019916134 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0884      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 218   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017211638 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0166      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00478    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020258773 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0974      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0167      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.009      |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=535.72 +/- 161.79\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 536         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018982526 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0922      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0278      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 221   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 119   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 227          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0123917125 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.0861       |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.00566      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00502     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=543.60 +/- 209.29\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 544         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018076718 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0814      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00549    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 219   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 140   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015341267 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0936      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0277      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00689    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015534259 |\n",
      "|    clip_fraction        | 0.0206      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=455.68 +/- 211.50\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 456         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011106228 |\n",
      "|    clip_fraction        | 0.00571     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0955      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0436      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 221   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 166   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021128815 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0936      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00917    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.70 +/- 107.63\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 501         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013075437 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 219   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 186   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012225722 |\n",
      "|    clip_fraction        | 0.00845     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0998      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=447.04 +/- 202.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 447         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020162642 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0743      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 218   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 206   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014538175 |\n",
      "|    clip_fraction        | 0.0179      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0752      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00959     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020720586 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=458.80 +/- 150.70\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 459         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017858002 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0934      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000665   |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 220   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 232   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023104768 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00919    |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=477.29 +/- 154.10\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 477         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018021844 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0967      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00357     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 218   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 252   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014732353 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0852      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00699    |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013052575 |\n",
      "|    clip_fraction        | 0.0143      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0763      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=594.66 +/- 245.70\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 595         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018412674 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.098       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0247      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 219   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 280   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013602837 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0926      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0307      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=643.77 +/- 208.49\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 644        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 65000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01811205 |\n",
      "|    clip_fraction        | 0.0292     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.77      |\n",
      "|    explained_variance   | 0.0957     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00817   |\n",
      "|    std                  | 0.965      |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 217   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 300   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 307         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018979855 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0917      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00879    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 313        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01255026 |\n",
      "|    clip_fraction        | 0.0181     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.74      |\n",
      "|    explained_variance   | 0.0964     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0285     |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.00632   |\n",
      "|    std                  | 0.948      |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=648.62 +/- 213.95\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 649         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022087645 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.0959      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00718     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00996    |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 218   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 327   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012302069 |\n",
      "|    clip_fraction        | 0.0131      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00271     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    std                  | 0.932       |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=550.54 +/- 179.69\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 551         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013257859 |\n",
      "|    clip_fraction        | 0.0165      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    std                  | 0.929       |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 217   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 348   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 355        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01579645 |\n",
      "|    clip_fraction        | 0.0241     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.69      |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0439     |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.00609   |\n",
      "|    std                  | 0.929      |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022163698 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.0953      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.000707    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00987    |\n",
      "|    std                  | 0.922       |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=514.14 +/- 205.79\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 514         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013762776 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00115     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 216   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 377   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 384         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019987078 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.0999      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0266      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00925    |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=593.90 +/- 194.67\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 594         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019350938 |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    std                  | 0.92        |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 215   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 399   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013969858 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.029       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    std                  | 0.919       |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=559.08 +/- 217.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 559        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 90000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01102584 |\n",
      "|    clip_fraction        | 0.0102     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.67      |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00434   |\n",
      "|    std                  | 0.922      |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 213   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 421   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023037955 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    std                  | 0.915       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019366607 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.0974      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=400.03 +/- 86.64\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 400         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023257427 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0181      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    std                  | 0.907       |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 214   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 449   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 455        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01735971 |\n",
      "|    clip_fraction        | 0.0274     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.65      |\n",
      "|    explained_variance   | 0.0843     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0424     |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.00678   |\n",
      "|    std                  | 0.911      |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=671.37 +/- 175.60\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 671        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01676374 |\n",
      "|    clip_fraction        | 0.0247     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.64      |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00765   |\n",
      "|    std                  | 0.898      |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 213    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 470    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 477         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021763891 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.0959      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.891       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 483        |\n",
      "|    total_timesteps      | 104448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902518 |\n",
      "|    clip_fraction        | 0.0219     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.6       |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 7.63e-05   |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.00849   |\n",
      "|    std                  | 0.883      |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=478.70 +/- 166.24\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022729032 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.882       |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 213    |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 498    |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 505        |\n",
      "|    total_timesteps      | 108544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01766872 |\n",
      "|    clip_fraction        | 0.0257     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | 0.0884     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00574    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.00689   |\n",
      "|    std                  | 0.877      |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=462.29 +/- 138.44\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 462         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014933979 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    std                  | 0.874       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 520    |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 526          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121230725 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.106        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.0191       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00678     |\n",
      "|    std                  | 0.872        |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016880807 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0211      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00793    |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=455.01 +/- 170.10\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 455        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 115000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01890421 |\n",
      "|    clip_fraction        | 0.0257     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.55      |\n",
      "|    explained_variance   | 0.092      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00442    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.00965   |\n",
      "|    std                  | 0.862      |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 548    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 554         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022269335 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.093       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00526     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00867    |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=511.57 +/- 138.94\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 512         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021456394 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00519     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.86        |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 570    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021372119 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    std                  | 0.851       |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 582         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015884183 |\n",
      "|    clip_fraction        | 0.0183      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=634.41 +/- 181.51\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 634         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017085161 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.0984      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00482     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 597    |\n",
      "|    total_timesteps | 126976 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 603         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016946197 |\n",
      "|    clip_fraction        | 0.0297      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.0922      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00317     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00815    |\n",
      "|    std                  | 0.839       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=618.58 +/- 286.99\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 619         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014189584 |\n",
      "|    clip_fraction        | 0.021       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    std                  | 0.84        |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 618    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 624         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025659584 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.831       |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=561.66 +/- 205.61\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 562         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017651085 |\n",
      "|    clip_fraction        | 0.0337      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    std                  | 0.83        |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 638    |\n",
      "|    total_timesteps | 135168 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011345651 |\n",
      "|    clip_fraction        | 0.0122      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.826       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016334277 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 0.0993      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0118      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    std                  | 0.823       |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=692.81 +/- 225.73\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 693         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011898592 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.44       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0226      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 666    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 672         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018472202 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.44       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0191      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=656.50 +/- 213.64\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 657         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020779327 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.43       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00937    |\n",
      "|    std                  | 0.811       |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 686    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 693         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021662392 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0163      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00891    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020503145 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.811       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=432.57 +/- 66.60\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 433         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016086668 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 714    |\n",
      "|    total_timesteps | 151552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 720         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021453394 |\n",
      "|    clip_fraction        | 0.0431      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.811       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=423.40 +/- 175.03\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 423        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 155000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01411464 |\n",
      "|    clip_fraction        | 0.0238     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.41      |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0155     |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.00815   |\n",
      "|    std                  | 0.807      |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 734    |\n",
      "|    total_timesteps | 155648 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 741         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021937419 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00372     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 747         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016772501 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00934    |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=409.67 +/- 64.43\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 410         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019238796 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.44       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00913     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 212    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 763    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 769         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018663377 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00966     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=391.84 +/- 202.38\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 392         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 165000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019375946 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 784    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 791         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012366524 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0349      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    std                  | 0.837       |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 797        |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01765361 |\n",
      "|    clip_fraction        | 0.0361     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.48      |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0338     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.00905   |\n",
      "|    std                  | 0.839      |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=422.26 +/- 102.28\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 422         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022080436 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.0906      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00695    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.846       |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 812    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 819         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023452943 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.000172    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00892    |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=402.26 +/- 101.62\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 402         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 175000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019945906 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00369    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 211    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 834    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 840         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016075233 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00943    |\n",
      "|    std                  | 0.822       |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=617.45 +/- 216.86\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 617         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027415514 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.44       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    std                  | 0.816       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 855    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 862        |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01958987 |\n",
      "|    clip_fraction        | 0.0367     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.43      |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00345    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.816      |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 868         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023966014 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.0976      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00457     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.803       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=521.19 +/- 153.54\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 521         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015168877 |\n",
      "|    clip_fraction        | 0.0182      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0148      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00841    |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 883    |\n",
      "|    total_timesteps | 186368 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 890         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015333958 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00532     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=482.77 +/- 169.96\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013595297 |\n",
      "|    clip_fraction        | 0.0196      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.0985      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000946   |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    std                  | 0.795       |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 905    |\n",
      "|    total_timesteps | 190464 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 912         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021225348 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00976    |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 918         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014721056 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=515.32 +/- 263.19\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 515         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019805666 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 933    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 940         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012207748 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.022       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00776    |\n",
      "|    std                  | 0.793       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=484.45 +/- 131.54\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 484        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 200000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01561431 |\n",
      "|    clip_fraction        | 0.0308     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.38      |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0229     |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.00924   |\n",
      "|    std                  | 0.796      |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 210    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 955    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_variant(RideHailingEnv_RPI, train_df, val_df, \"model_rpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard_logs/model_rpi_dpi/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 353  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015441262 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.255      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0177      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.295       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=464.03 +/- 74.11\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 464         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024124354 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0777      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000361   |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 221  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 27   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014761396 |\n",
      "|    clip_fraction        | 0.0111      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00377     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=527.84 +/- 168.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 528         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016712824 |\n",
      "|    clip_fraction        | 0.0199      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0896      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00654     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00696    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 202   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012997087 |\n",
      "|    clip_fraction        | 0.0138      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0732      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00526    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013559781 |\n",
      "|    clip_fraction        | 0.0125      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.0803      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00784     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=448.91 +/- 65.40\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 449         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017847663 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00712    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00731    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025091164 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0739      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00204    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=663.53 +/- 227.65\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 664         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026460823 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0831      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 202   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 101   |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019535646 |\n",
      "|    clip_fraction        | 0.0267      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0864      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00591     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00835    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014982795 |\n",
      "|    clip_fraction        | 0.0117      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=561.31 +/- 218.69\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 561         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016550897 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.092       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0124      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 129   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012680151 |\n",
      "|    clip_fraction        | 0.0118      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.0867      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00533     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=606.23 +/- 141.81\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 606         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009651806 |\n",
      "|    clip_fraction        | 0.0103      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.0769      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00445     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 150   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016318742 |\n",
      "|    clip_fraction        | 0.0231      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017468017 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.092       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000643   |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00895    |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=664.49 +/- 201.98\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 664         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020875365 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0819      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00879    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 179   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017056124 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.0809      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=597.23 +/- 207.30\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 597         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013550691 |\n",
      "|    clip_fraction        | 0.0125      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.0898      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0219      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 201   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 207          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0148410015 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.00893      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00741     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 0.146        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=370.19 +/- 48.24\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 370         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017226793 |\n",
      "|    clip_fraction        | 0.021       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0864      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0117      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 202   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 222   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013386365 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0826      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 0.963       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 235        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01625284 |\n",
      "|    clip_fraction        | 0.022      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00294    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00671   |\n",
      "|    std                  | 0.964      |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=581.04 +/- 234.86\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 581         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015913308 |\n",
      "|    clip_fraction        | 0.0172      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0889      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00213    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00634    |\n",
      "|    std                  | 0.961       |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 204   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 250   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015475336 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0864      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00246     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=593.51 +/- 215.01\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 594         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021471443 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.0957      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00974    |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 272   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022005653 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.0949      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0164      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 285          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108158225 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.103        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | -0.00229     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    std                  | 0.946        |\n",
      "|    value_loss           | 0.113        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=456.77 +/- 96.67\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018524617 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00826    |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 204   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 300   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017646607 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0919      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00937    |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=499.65 +/- 219.70\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 65000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01988835 |\n",
      "|    clip_fraction        | 0.0326     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.093      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.964      |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 321   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008775935 |\n",
      "|    clip_fraction        | 0.0105      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0839      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00672     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 0.961       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018337265 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0928      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00723    |\n",
      "|    std                  | 0.957       |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=353.23 +/- 55.31\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 353         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020445641 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0984      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00286    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0081     |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 349   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012469353 |\n",
      "|    clip_fraction        | 0.0084      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0946      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=351.01 +/- 47.48\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 351         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017227683 |\n",
      "|    clip_fraction        | 0.0224      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00443     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 371   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 378         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019980144 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.096       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.948       |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 207       |\n",
      "|    iterations           | 39        |\n",
      "|    time_elapsed         | 384       |\n",
      "|    total_timesteps      | 79872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0192599 |\n",
      "|    clip_fraction        | 0.034     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -2.71     |\n",
      "|    explained_variance   | 0.0999    |\n",
      "|    learning_rate        | 0.0004    |\n",
      "|    loss                 | 0.0268    |\n",
      "|    n_updates            | 380       |\n",
      "|    policy_gradient_loss | -0.0129   |\n",
      "|    std                  | 0.932     |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=539.38 +/- 190.56\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 539        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01698343 |\n",
      "|    clip_fraction        | 0.025      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.68      |\n",
      "|    explained_variance   | 0.0984     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.00828   |\n",
      "|    std                  | 0.922      |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 204   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 399   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024246339 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.0957      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=624.10 +/- 229.53\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 624         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018028058 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00451    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 204   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 421   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 427        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02801513 |\n",
      "|    clip_fraction        | 0.049      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.66      |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0198     |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.912      |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=473.67 +/- 241.79\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018541325 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00798    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    std                  | 0.908       |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 442   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015383867 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    std                  | 0.911       |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021840744 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.0844      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.000793   |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=566.37 +/- 162.75\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 566         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025092278 |\n",
      "|    clip_fraction        | 0.0454      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.0993      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 204   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 470   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 477         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019698124 |\n",
      "|    clip_fraction        | 0.0313      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.0982      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0392      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=625.24 +/- 205.23\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 625         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022116682 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.0983      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 492    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 498         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018379457 |\n",
      "|    clip_fraction        | 0.0301      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.0989      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00911    |\n",
      "|    std                  | 0.902       |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018011138 |\n",
      "|    clip_fraction        | 0.0209      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00847     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=315.20 +/- 37.56\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 315         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019280922 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00567     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    std                  | 0.891       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 520    |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 206       |\n",
      "|    iterations           | 53        |\n",
      "|    time_elapsed         | 526       |\n",
      "|    total_timesteps      | 108544    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0257831 |\n",
      "|    clip_fraction        | 0.0515    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -2.59     |\n",
      "|    explained_variance   | 0.0949    |\n",
      "|    learning_rate        | 0.0004    |\n",
      "|    loss                 | 0.0208    |\n",
      "|    n_updates            | 520       |\n",
      "|    policy_gradient_loss | -0.0147   |\n",
      "|    std                  | 0.88      |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=393.04 +/- 156.41\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 393         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017950436 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    std                  | 0.881       |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 541    |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 548        |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02364232 |\n",
      "|    clip_fraction        | 0.0488     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | 0.089      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0219     |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.885      |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 554         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016642876 |\n",
      "|    clip_fraction        | 0.026       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | 0.0964      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00831    |\n",
      "|    std                  | 0.883       |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=438.96 +/- 156.86\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 439        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 115000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02140611 |\n",
      "|    clip_fraction        | 0.0477     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | 0.102      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.878      |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 569    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020146422 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.87        |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=615.20 +/- 222.42\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 615         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011516835 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    std                  | 0.867       |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 591    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014360918 |\n",
      "|    clip_fraction        | 0.0206      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00764    |\n",
      "|    std                  | 0.87        |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 603         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015387796 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00982    |\n",
      "|    std                  | 0.87        |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=470.04 +/- 197.78\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016155427 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    std                  | 0.881       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 205    |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 618    |\n",
      "|    total_timesteps | 126976 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 206        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 625        |\n",
      "|    total_timesteps      | 129024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02362468 |\n",
      "|    clip_fraction        | 0.0484     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | 0.102      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0358     |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.875      |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=434.59 +/- 230.06\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 435         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010684778 |\n",
      "|    clip_fraction        | 0.0153      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 640    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 646         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018659044 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.871       |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=602.91 +/- 214.95\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 603         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017273892 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.0966      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.868       |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 661    |\n",
      "|    total_timesteps | 135168 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016312754 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00959    |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010545015 |\n",
      "|    clip_fraction        | 0.0103      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=475.25 +/- 207.02\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 475         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015164243 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00622    |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 690    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022450242 |\n",
      "|    clip_fraction        | 0.0408      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0252      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=520.27 +/- 144.00\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 520         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021424573 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.007       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.868       |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 711    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015309207 |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00471    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    std                  | 0.868       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 724         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030678859 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    std                  | 0.861       |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=313.17 +/- 73.04\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 313         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020529158 |\n",
      "|    clip_fraction        | 0.0328      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 739    |\n",
      "|    total_timesteps | 151552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013784365 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.0892      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00177    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    std                  | 0.859       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=581.74 +/- 188.75\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 582         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 155000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015232451 |\n",
      "|    clip_fraction        | 0.0173      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 764    |\n",
      "|    total_timesteps | 155648 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 771         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021960061 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00758     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.859       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015348497 |\n",
      "|    clip_fraction        | 0.0191      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00424     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    std                  | 0.852       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=443.70 +/- 173.50\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 444        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02192989 |\n",
      "|    clip_fraction        | 0.0478     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.51      |\n",
      "|    explained_variance   | 0.0973     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | -0.0175    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.847      |\n",
      "|    value_loss           | 0.137      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 794    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 801         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017405223 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.003       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    std                  | 0.835       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=478.57 +/- 211.21\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 165000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016158652 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00885    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00929    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 815    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 821         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017426362 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0163      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 828         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017280884 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00708     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00998    |\n",
      "|    std                  | 0.81        |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=550.61 +/- 239.92\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 551         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012450218 |\n",
      "|    clip_fraction        | 0.0126      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    std                  | 0.799       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 842    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 848        |\n",
      "|    total_timesteps      | 174080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02531406 |\n",
      "|    clip_fraction        | 0.0585     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.39      |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | -0.000192  |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.804      |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=562.87 +/- 188.10\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 563        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 175000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01814484 |\n",
      "|    clip_fraction        | 0.0256     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.4       |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.00947   |\n",
      "|    std                  | 0.799      |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 862    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 869         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019826977 |\n",
      "|    clip_fraction        | 0.0337      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.0905      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.02        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0093     |\n",
      "|    std                  | 0.807       |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=523.90 +/- 203.20\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 524         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023209698 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.8         |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 885    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 891         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017942648 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00583     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 898         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013983216 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0118      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00951    |\n",
      "|    std                  | 0.791       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=564.32 +/- 207.84\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 564         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015999913 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 913    |\n",
      "|    total_timesteps | 186368 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 920         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020211965 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00837     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.779       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=480.44 +/- 168.88\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 480         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026048098 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.34       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00714     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.779       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 935    |\n",
      "|    total_timesteps | 190464 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016423957 |\n",
      "|    clip_fraction        | 0.0204      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    std                  | 0.775       |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 948         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016650522 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00921    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    std                  | 0.771       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=463.43 +/- 144.83\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 463         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017163493 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.0985      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    std                  | 0.774       |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 204    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 963    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 969         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022722375 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.0983      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00665     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=475.00 +/- 201.60\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 475         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023068689 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0293      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.77        |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 203    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 984    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_variant(RideHailingEnv_RPI_DPI, train_df, val_df, \"model_rpi_dpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard_logs/model_hdf/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 352  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 336         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013849928 |\n",
      "|    clip_fraction        | 0.00688     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | -0.193      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0826      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00521    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.274       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=595.42 +/- 230.23\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 595         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022885673 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.057       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 224  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 27   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013850538 |\n",
      "|    clip_fraction        | 0.0147      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.0759      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=659.89 +/- 214.66\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 660         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016061127 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.088       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 209   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 222          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128560215 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.058        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.0228       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00493     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.161        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016122757 |\n",
      "|    clip_fraction        | 0.0132      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0722      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0287      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=493.56 +/- 157.60\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 494         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016817141 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0633      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 213   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021889396 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0811      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=548.81 +/- 188.79\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 549         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017085597 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0588      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.022       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 208   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020534787 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0583      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0187      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0081     |\n",
      "|    std                  | 0.963       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 110        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01793322 |\n",
      "|    clip_fraction        | 0.0256     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.0837     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00723   |\n",
      "|    std                  | 0.964      |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=392.03 +/- 86.34\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 392         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014085326 |\n",
      "|    clip_fraction        | 0.0167      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0968      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 211   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 125   |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019999411 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0857      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00852    |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=618.20 +/- 231.52\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 618        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02379149 |\n",
      "|    clip_fraction        | 0.0492     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.74      |\n",
      "|    explained_variance   | 0.078      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0288     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.947      |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 208   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 147   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015334312 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.0784      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00773    |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019602966 |\n",
      "|    clip_fraction        | 0.0328      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.0923      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    std                  | 0.948       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=451.21 +/- 157.35\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 451        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01972846 |\n",
      "|    clip_fraction        | 0.0347     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.74      |\n",
      "|    explained_variance   | 0.1        |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00643    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.00908   |\n",
      "|    std                  | 0.957      |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 209   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 175   |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145810265 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.0937       |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.0285       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00676     |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=502.18 +/- 248.20\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 502         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026286967 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0857      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 207   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 197   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017756011 |\n",
      "|    clip_fraction        | 0.0235      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0877      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.961       |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=490.12 +/- 69.17\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 490         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017805766 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0869      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 206   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 218   |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013463857 |\n",
      "|    clip_fraction        | 0.0223      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.089       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0166      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 231        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02061812 |\n",
      "|    clip_fraction        | 0.0268     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.0902     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | -0.00349   |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.963      |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=417.67 +/- 138.39\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 418         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014756559 |\n",
      "|    clip_fraction        | 0.0211      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0939      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00622     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 207   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 246   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 252          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155835925 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.0904       |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.0285       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00879     |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 0.174        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=556.26 +/- 240.90\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 556         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017802862 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00767     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00855    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 206   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 267   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015912801 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00971     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 280        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01488656 |\n",
      "|    clip_fraction        | 0.019      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.73      |\n",
      "|    explained_variance   | 0.0919     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0376     |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0064    |\n",
      "|    std                  | 0.949      |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=370.97 +/- 197.43\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018227091 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 207   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 295   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 302         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015867371 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0854      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00433     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=548.13 +/- 170.77\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 548         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018190842 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 206   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 316   |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015794173 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00656    |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022576086 |\n",
      "|    clip_fraction        | 0.0372      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00445     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=612.27 +/- 215.32\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 612        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02147377 |\n",
      "|    clip_fraction        | 0.0293     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.74      |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | -0.019     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.00778   |\n",
      "|    std                  | 0.957      |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 207   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 344   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023164742 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0891      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00167    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 0.0992      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=433.14 +/- 168.07\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 433         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014500675 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.0959      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0316      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 207   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 366   |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020057373 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0982      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00475     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 379        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01956742 |\n",
      "|    clip_fraction        | 0.0285     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.102      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0269     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.00871   |\n",
      "|    std                  | 0.959      |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=464.19 +/- 213.35\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 464         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025311759 |\n",
      "|    clip_fraction        | 0.0472      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.093       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0252      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 207   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 393   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016234213 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00627    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=502.84 +/- 166.05\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 503         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017631043 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.0955      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0166      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 206   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 415   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014211547 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0885      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=518.21 +/- 181.73\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 518         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019035272 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00797     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00942    |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 437   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017861802 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00927     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00777    |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 450          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141626205 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.103        |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | 0.0293       |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00854     |\n",
      "|    std                  | 0.945        |\n",
      "|    value_loss           | 0.174        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=504.51 +/- 172.88\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 505         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021276375 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 206   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 465   |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 471        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02387843 |\n",
      "|    clip_fraction        | 0.0364     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.72      |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.00869   |\n",
      "|    std                  | 0.941      |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=472.94 +/- 157.49\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 473         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023156267 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.939       |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 486    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023226032 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00959     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 499         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020029318 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.0978      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0217      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00777    |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=604.28 +/- 204.14\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 604        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 105000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01827605 |\n",
      "|    clip_fraction        | 0.0216     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.71      |\n",
      "|    explained_variance   | 0.0917     |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.00558    |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00694   |\n",
      "|    std                  | 0.936      |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 514    |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018753862 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00988     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00855    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=425.42 +/- 169.28\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 425         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018472869 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.0971      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.923       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 535    |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030937757 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00332    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    std                  | 0.925       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017276987 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.0925      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=634.18 +/- 230.53\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 634        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 115000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02082127 |\n",
      "|    clip_fraction        | 0.0373     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.67      |\n",
      "|    explained_variance   | 0.1        |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.000869   |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.915      |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 563    |\n",
      "|    total_timesteps | 116736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 569         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019728513 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0309      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00824    |\n",
      "|    std                  | 0.911       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=489.46 +/- 174.22\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 489         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023245914 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0321      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 584    |\n",
      "|    total_timesteps | 120832 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 590         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022306403 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00776     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022512423 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.092       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00516     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=561.65 +/- 208.78\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 562         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017985739 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0088     |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 612    |\n",
      "|    total_timesteps | 126976 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013890283 |\n",
      "|    clip_fraction        | 0.0176      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    std                  | 0.883       |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=588.45 +/- 144.05\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 588         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014808435 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00826     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    std                  | 0.883       |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 633    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018683262 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=413.26 +/- 74.41\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 413         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018083563 |\n",
      "|    clip_fraction        | 0.0345      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0266      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 655    |\n",
      "|    total_timesteps | 135168 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022367677 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0206      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025191639 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00992     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00974    |\n",
      "|    std                  | 0.848       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=461.13 +/- 234.09\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 461         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019943252 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.01        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00892    |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 683    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 689         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015865978 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0425      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0087     |\n",
      "|    std                  | 0.846       |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=457.44 +/- 151.74\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021121185 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.0935      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00741     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 704    |\n",
      "|    total_timesteps | 145408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021183357 |\n",
      "|    clip_fraction        | 0.0398      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0216      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015604591 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00755    |\n",
      "|    std                  | 0.849       |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=530.24 +/- 219.50\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 530         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023453018 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.096       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0288      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 732    |\n",
      "|    total_timesteps | 151552 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 739        |\n",
      "|    total_timesteps      | 153600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02206593 |\n",
      "|    clip_fraction        | 0.0403     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.51      |\n",
      "|    explained_variance   | 0.11       |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0333     |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.853      |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=397.51 +/- 111.41\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 398         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 155000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025993261 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.0977      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00899    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    std                  | 0.853       |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 754    |\n",
      "|    total_timesteps | 155648 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 760          |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0153535865 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.0004       |\n",
      "|    loss                 | -0.00186     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    std                  | 0.842        |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 767         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021936502 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.832       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=520.63 +/- 180.67\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 521         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025966479 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.0959      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0139      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0099     |\n",
      "|    std                  | 0.835       |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 782    |\n",
      "|    total_timesteps | 161792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023189098 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.834       |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=406.16 +/- 100.39\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 406         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 165000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020516066 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 803    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 809         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021924146 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.023       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 816         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023243856 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=556.49 +/- 188.75\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 556         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021642776 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.00117    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 830    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017485842 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.0982      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00884     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.837       |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=434.56 +/- 150.45\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 435         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 175000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018891068 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0162      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.84        |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 852    |\n",
      "|    total_timesteps | 176128 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 858        |\n",
      "|    total_timesteps      | 178176     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02478074 |\n",
      "|    clip_fraction        | 0.0583     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.48      |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0037     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.833      |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=524.08 +/- 163.93\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 524         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014441664 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0239      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    std                  | 0.839       |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 206    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 872    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 207       |\n",
      "|    iterations           | 89        |\n",
      "|    time_elapsed         | 878       |\n",
      "|    total_timesteps      | 182272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0170294 |\n",
      "|    clip_fraction        | 0.0315    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -2.49     |\n",
      "|    explained_variance   | 0.11      |\n",
      "|    learning_rate        | 0.0004    |\n",
      "|    loss                 | 0.0309    |\n",
      "|    n_updates            | 880       |\n",
      "|    policy_gradient_loss | -0.0095   |\n",
      "|    std                  | 0.842     |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 885        |\n",
      "|    total_timesteps      | 184320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01731556 |\n",
      "|    clip_fraction        | 0.0253     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.49      |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.00734   |\n",
      "|    std                  | 0.837      |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=377.45 +/- 106.16\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 377         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016022418 |\n",
      "|    clip_fraction        | 0.0174      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00119     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00856    |\n",
      "|    std                  | 0.835       |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 899    |\n",
      "|    total_timesteps | 186368 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 905        |\n",
      "|    total_timesteps      | 188416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02773279 |\n",
      "|    clip_fraction        | 0.0511     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.48      |\n",
      "|    explained_variance   | 0.109      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 0.0218     |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.833      |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=430.17 +/- 173.16\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 430         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015411176 |\n",
      "|    clip_fraction        | 0.0211      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00767    |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 919    |\n",
      "|    total_timesteps | 190464 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 925         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017533652 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.00319     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.834       |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 931         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023335017 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.834       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=525.34 +/- 230.46\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 525         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019086348 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    std                  | 0.83        |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 946    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 952         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022195771 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.83        |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=624.15 +/- 189.27\n",
      "Episode length: 1001.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 624         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025899163 |\n",
      "|    clip_fraction        | 0.0488      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 207    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 966    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_variant(RideHailingEnv_HDF, train_df, val_df, \"model_hdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def evaluate_model(model_path, vecnormalize_path, env_class, test_df):\n",
    "    # Create test env with Monitor to support VecNormalize\n",
    "    env = DummyVecEnv([lambda: Monitor(env_class(test_df))])\n",
    "\n",
    "    # Load VecNormalize wrapper with correct environment\n",
    "    vec_norm = VecNormalize.load(vecnormalize_path, env)\n",
    "    vec_norm.training = False\n",
    "    vec_norm.norm_reward = False\n",
    "\n",
    "    #  Load the PPO model\n",
    "    model = PPO.load(model_path)\n",
    "\n",
    "    obs = vec_norm.reset()\n",
    "    total_cost = 0\n",
    "    completed_rides = 0\n",
    "    total_cr = 0\n",
    "    num_steps = 0\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_norm.step(action)\n",
    "\n",
    "        cr = info[0].get(\"CR\", 0.5)\n",
    "        total_cr += cr\n",
    "        if cr < 0.5:\n",
    "            completed_rides += 1\n",
    "\n",
    "        fare_adjustment, rider_incentive = action[0]\n",
    "        base_fare = test_df.iloc[num_steps].get(\"Base Fare\", 10.0)\n",
    "        delta = 0.2 * base_fare\n",
    "        cost = rider_incentive + abs(fare_adjustment) * delta\n",
    "        total_cost += cost\n",
    "\n",
    "        num_steps += 1\n",
    "        if num_steps >= len(test_df) or done:\n",
    "            break\n",
    "\n",
    "    avg_cr = total_cr / num_steps\n",
    "    avg_cost = total_cost / num_steps\n",
    "    completion_rate = completed_rides / num_steps\n",
    "\n",
    "    return {\n",
    "        \"avg_cost\": avg_cost,\n",
    "        \"completion_rate\": completion_rate,\n",
    "        \"avg_cr\": avg_cr,\n",
    "        \"total_steps\": num_steps\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"datasets/test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\analysis\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "model_variants = {\n",
    "    \"Base\": (\"models/model_base.zip\", \"models/model_base_vecnormalize.pkl\", RideHailingEnv_Base),\n",
    "    \"RPI\": (\"models/model_rpi.zip\", \"models/model_rpi_vecnormalize.pkl\", RideHailingEnv_RPI),\n",
    "    \"RPI+DPI\": (\"models/model_rpi_dpi.zip\", \"models/model_rpi_dpi_vecnormalize.pkl\", RideHailingEnv_RPI_DPI),\n",
    "    \"HDF\": (\"models/model_hdf.zip\", \"models/model_hdf_vecnormalize.pkl\", RideHailingEnv_HDF),\n",
    "    \"DWF\": (\"models/dwf_model.zip\", \"models/dwf_vecnormalize.pkl\", RideHailingEnv_DWF)\n",
    "}\n",
    "\n",
    "for name, (model_path, vecnorm_path, env_class) in model_variants.items():\n",
    "    results[name] = evaluate_model(model_path, vecnorm_path, env_class, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(results.keys())\n",
    "avg_costs = [results[k][\"avg_cost\"] for k in labels]\n",
    "completion_rates = [results[k][\"completion_rate\"] for k in labels]\n",
    "cancel_rates = [results[k][\"avg_cr\"] for k in labels]\n",
    "\n",
    "# Plot Cost\n",
    "plt.figure()\n",
    "plt.bar(labels, avg_costs)\n",
    "plt.ylabel(\"Avg Cost per Ride\")\n",
    "plt.title(\"Model Comparison: Avg Incentive Cost\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Completion Rate\n",
    "plt.figure()\n",
    "plt.bar(labels, completion_rates)\n",
    "plt.ylabel(\"Completion Rate\")\n",
    "plt.title(\"Model Comparison: Completion Rate\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Cancellation Rate\n",
    "plt.figure()\n",
    "plt.bar(labels, cancel_rates)\n",
    "plt.ylabel(\"Average CR\")\n",
    "plt.title(\"Model Comparison: Average Cancellation Rate (CR)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
